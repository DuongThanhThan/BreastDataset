{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BreastDataset.ipynb","provenance":[{"file_id":"1BQNKPcC5V0Zxg6C9ecKODFejTRTL4d3p","timestamp":1616928404677}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"aoJOmNNqSVWy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621690641892,"user_tz":-420,"elapsed":20503,"user":{"displayName":"Duong Thanh Than","photoUrl":"","userId":"01747969036272990033"}},"outputId":"62c70cba-3594-4f03-8287-29f71598362f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"asFU8ZyxrdXb"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"zY15uq7mpITC"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import os\n","import cv2 \n","import random\n","import pickle\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, AveragePooling2D\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozz8v9Ajsrxo"},"source":["data_dir = \"/content/drive/MyDrive/BreastDataset\"\n","data_dir = Path(data_dir)  \n","categories = os.listdir(data_dir)\n","categories.remove(\"InSitu\")\n","categories.remove(\".ipynb_checkpoints\")\n","categories.remove(\"127.0.0.1_5000 - Google Chrome 2021-04-04 14-40-05.mp4\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IstOJFKM_PUX","executionInfo":{"status":"ok","timestamp":1621690658335,"user_tz":-420,"elapsed":1074,"user":{"displayName":"Duong Thanh Than","photoUrl":"","userId":"01747969036272990033"}},"outputId":"fe5548a8-b83d-430e-9428-66926bae79b7"},"source":["count_image = len(list(data_dir.glob(\"*/*.tif\")))\n","print(count_image)\n","print(len(categories))\n","print(categories)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1150\n","3\n","['Normal', 'Invasive', 'Benign']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CttdJM0S_zPT"},"source":["training_data = []  # Toàn bộ dữ liệu chúng ta có thể sử dụng, phân biệt giữa training data, x_train, y_train, x_test, y_test, x_val và y_val\n","# training data = [x_train, y_train] + [x_test, y_test] + [x_val, y_val]\n","\n","IMG_SIZE = (1000, 750)  # Trong hướng dẫn này, lựa chọn đưa toàn bộ ảnh về cùng kích thước 200 x 200, (bộ dữ liệu với nhiều loại kích thước ảnh)\n","def create_training_data():\n","  for category in categories: # Duyệt qua từng folder label\n","    path = os.path.join(data_dir, category) # tạo một path tạm đến folder label đó \n","    label_number = categories.index(category) # label thứ mấy trong mảng categories chứa các label\n","    for img in os.listdir(path):  # duyệt qua toàn bộ ảnh trong path\n","      try:\n","        img_array_gray = cv2.imread(os.path.join(path, img), cv2.IMREAD_ANYCOLOR) # Đọc mảng giá trị của ảnh, ở đây không chuyển ảnh thành gray\n","        new_img_array = cv2.resize(img_array_gray, IMG_SIZE, interpolation = cv2.INTER_AREA)  # Chuyển ảnh về cùng kích thước\n","        training_data.append([new_img_array, label_number]) # Lưu ảnh cùng label tương ứng vào training_data\n","      except Exception as e:\n","        pass\n","\n","create_training_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUnUp6gA_4Aw"},"source":["random.shuffle(training_data) # xáo trộn dữ liệu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XuiIFOCAWNE"},"source":["# Tạo 2 mảng để lưu samples và labels theo cùng index tương ứng\n","X = []  # samples\n","y = []  # labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dvig76z0Aa0Y"},"source":["for feature, label in training_data:  # Mỗi phần tử trong training_data có dạng [feature, label] trong đó feature = new_img_array và label = label_number\n","  X.append(feature)\n","  y.append(label)\n","\n","# X bây giờ chứa toàn bộ features\n","# Chuyển X về dạng ma trận\n","# -1 ở đây tương đương với số lượng features, dùng -1 numpy tự hiểu là ứng với số lượng của feature hiện có trong X\n","# Vì không chuyển thành ảnh trắng đen nên ảnh có 3 kênh màu, tham số cuối cùng trong reshape\n","X = np.array(X).reshape(-1, IMG_SIZE[1], IMG_SIZE[0], 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVhZPUJyBPOd"},"source":["# Bước này dùng để lưu X và y vào một file theo định dạng có cấu trúc để khỏi phải chạy lại hàm tạo X, y phía trên - khá tốn thời gian cho việc tạo lại từ đầu\n","pickle_out = open(\"/content/drive/MyDrive/Colab Notebooks/X.pickle\", \"wb\")\n","pickle.dump(X, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(\"/content/drive/MyDrive/Colab Notebooks/y.pickle\", \"wb\")\n","pickle.dump(y, pickle_out)\n","pickle_out.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzBCHcQpBQE-"},"source":["# Đọc lại file cấu trúc đã lưu\n","pickle_in = open(\"/content/drive/MyDrive/Colab Notebooks/X_4.pickle\", \"rb\")\n","X = pickle.load(pickle_in)\n","pickle_in.close()\n","\n","pickle_in = open(\"/content/drive/MyDrive/Colab Notebooks/y_4.pickle\", \"rb\")\n","y = pickle.load(pickle_in)\n","pickle_in.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wN6Twr8cBfwl"},"source":["x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n","x_train, x_val, y_train, y_val =  train_test_split(x_train, y_train, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GG9rOjQulg9E"},"source":["x_train = x_train/255 \n","x_test = x_test/255\n","x_val = x_val/225\n","\n","y_train = to_categorical(y_train) # one hot\n","y_test = to_categorical(y_test)\n","y_val = to_categorical(y_val)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nP7-HHbjBxgc"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"deCxzLV6B0B8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617638862105,"user_tz":-420,"elapsed":1266433,"user":{"displayName":"Duong Thanh Than","photoUrl":"","userId":"01747969036272990033"}},"outputId":"be6626c7-1600-4720-d46b-c2b268a4ae97"},"source":["model = Sequential() #  Khởi tạo một một mô hình neural\n","\n","# Bắt đầu của mạng thêm một Convolutional layer với các tham số trên\n","# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n","# Lớp ban đầu này phải truyền cho mạng biết input_shape ban đầu là bao nhiêu chính là (weight x heigh x depth) của ảnh\n","# padding chỉ có thể chọn 1 trong 2 giá trị `same` hoặc `valid`\n","model.add(Conv2D(filters=16, kernel_size=(3,3), activation='tanh', input_shape=X.shape[1:], padding=\"same\"))\n","\n","model.add(AveragePooling2D(pool_size=(2, 2), strides=2))\n","\n","model.add(Conv2D(filters=32, kernel_size=(3,3), activation='tanh', padding=\"same\"))\n","\n","model.add(AveragePooling2D(pool_size=(2, 2), strides=2))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"))\n","\n","model.add(AveragePooling2D(pool_size=(2, 2), strides=2))\n","\n","model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"))\n","\n","model.add(AveragePooling2D(pool_size=(2, 2), strides=2))\n","\n","# Làm phẳng neuron, bắt đầu fully connected\n","model.add(Flatten())\n","# Tạo một layer\n","model.add(Dense(64))\n","model.add(Activation(\"relu\"))\n","model.add(Dense(64))\n","model.add(Activation(\"relu\"))\n","\n","# output\n","model.add(Dense(len(categories), activation='softmax'))\n","\n","# Cung cấp cho mô hình hàm loss function lựa chọn, thuật toán tối ưu\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","model.summary() # xem chi tiết mô hình xây dựng của chính mình\n","\n","# Cuối cùng đào tạo nó\n","model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=64, epochs=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 384, 512, 16)      448       \n","_________________________________________________________________\n","average_pooling2d (AveragePo (None, 192, 256, 16)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 192, 256, 32)      4640      \n","_________________________________________________________________\n","average_pooling2d_1 (Average (None, 96, 128, 32)       0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 96, 128, 64)       18496     \n","_________________________________________________________________\n","average_pooling2d_2 (Average (None, 48, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 48, 64, 64)        36928     \n","_________________________________________________________________\n","average_pooling2d_3 (Average (None, 24, 32, 64)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 49152)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                3145792   \n","_________________________________________________________________\n","activation (Activation)      (None, 64)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 3,210,659\n","Trainable params: 3,210,659\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/10\n","11/11 [==============================] - 129s 12s/step - loss: 1.2864 - accuracy: 0.3245 - val_loss: 1.0838 - val_accuracy: 0.3486\n","Epoch 2/10\n","11/11 [==============================] - 126s 11s/step - loss: 1.0880 - accuracy: 0.3798 - val_loss: 1.0790 - val_accuracy: 0.3714\n","Epoch 3/10\n","11/11 [==============================] - 126s 12s/step - loss: 1.0698 - accuracy: 0.4204 - val_loss: 1.0734 - val_accuracy: 0.4571\n","Epoch 4/10\n","11/11 [==============================] - 125s 11s/step - loss: 1.0490 - accuracy: 0.5116 - val_loss: 1.0710 - val_accuracy: 0.4971\n","Epoch 5/10\n","11/11 [==============================] - 126s 12s/step - loss: 1.0569 - accuracy: 0.4299 - val_loss: 1.0619 - val_accuracy: 0.4114\n","Epoch 6/10\n","11/11 [==============================] - 126s 11s/step - loss: 1.0203 - accuracy: 0.4570 - val_loss: 1.0896 - val_accuracy: 0.4343\n","Epoch 7/10\n","11/11 [==============================] - 128s 12s/step - loss: 1.0394 - accuracy: 0.4613 - val_loss: 1.0707 - val_accuracy: 0.5486\n","Epoch 8/10\n","11/11 [==============================] - 127s 12s/step - loss: 1.0668 - accuracy: 0.4741 - val_loss: 1.0668 - val_accuracy: 0.3771\n","Epoch 9/10\n","11/11 [==============================] - 126s 12s/step - loss: 1.0371 - accuracy: 0.4288 - val_loss: 1.0838 - val_accuracy: 0.4114\n","Epoch 10/10\n","11/11 [==============================] - 126s 12s/step - loss: 0.9862 - accuracy: 0.4801 - val_loss: 0.9930 - val_accuracy: 0.5657\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f1f24d51810>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"My-23IZ5Di99","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617639058029,"user_tz":-420,"elapsed":17206,"user":{"displayName":"Duong Thanh Than","photoUrl":"","userId":"01747969036272990033"}},"outputId":"39119c95-88a5-4619-d543-787de5d06274"},"source":["loss, acc = model.evaluate(x_test, y_test, verbose=2)\n","print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7/7 - 14s - loss: 0.9977 - accuracy: 0.5688\n","Restored model, accuracy: 56.88%\n"],"name":"stdout"}]}]}